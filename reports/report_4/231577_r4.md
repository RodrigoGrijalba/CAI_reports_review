### Statistical Challenges in Online Controlled Experiments: A Review of A/B Testing Methodology

The article addresses the practice, methodology, and literature concerning Online Controlled Experiments (OCEs), which virtualize Randomized Controlled Trials (RCTs), commonly known as A/B testing. These have become indispensable tools for technology companies in data-driven decision-making and optimizing online products and services.

Primarily, it focuses on addressing the main statistical challenges in implementing OCEs and how existing methodologies can be improved to achieve efficient outcomes.

One of the main strengths of the article is the methods discussed for analyzing the Average Treatment Effect (ATE), defined as the difference in the average outcome when both treatment and control are globally applied, represented by $t = E[Y(1) - Y(0)]$. Notably, it includes CUPED, which involves transforming $Y$ into $Y^*$ to reduce the variance of $t$, as well as using the same variable from past experiments to increase statistical significance.

On the other hand, a weakness is that although it is reasonable to assume that user responses do not depend on the treatment assignments of other users (SUTVA), the article does not delve deeply into instances where this assumption is violated (leakage), posing a problem for companies or applications where users interact over a network, such as social media platforms today.

The primary contribution of the paper is the presentation of real-life cases when introducing techniques to enhance experimental power and the heterogeneous and long-term effects of treatment and control groups. The effectiveness of the experiments has had positive impacts on company benefits.

Lastly, to continue progressing in the analysis of these topics, the authors call on industries and academic statisticians to address the challenges, especially regarding long-term estimation effects, due to the lack of statistical consensus for addressing this problem.
